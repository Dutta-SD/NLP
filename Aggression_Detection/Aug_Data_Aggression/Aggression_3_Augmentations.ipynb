{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aggression_3_Augmentations.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOAXNPoGA1QcIfMjvIKep/b"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JNJel8H9qjsC"
      },
      "source": [
        "# Data Augmentation for Aggression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lV9DkWFIWeiX"
      },
      "source": [
        "## ENABLE GPU BEFORE PROCEEDING WITH NOTEBOOK\n",
        "\n",
        "! pip3 install -qq transformers\n",
        "import transformers"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pY1DlOpGW4tv",
        "outputId": "2a8d58df-c86f-4e24-af76-4de0dfe69612"
      },
      "source": [
        "import transformers\n",
        "import numpy as np\n",
        "import nltk\n",
        "import pandas as pd\n",
        "from google.colab import files\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "\n",
        "# nltk setup\n",
        "nltk.download('stopwords')\n",
        "\n",
        "# Set random seed\n",
        "transformers.trainer_utils.set_seed(0)\n",
        "\n",
        "# Shut off warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TMowxnX54GuQ"
      },
      "source": [
        "# Set augmentation parameters\n",
        "# DATA SPECIFIC PARAMETERS\n",
        "\n",
        "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "aug_map = {\n",
        "    'task' : 'Sub-task B',\n",
        "    'map' : {\n",
        "        'NGEN' : 0,\n",
        "        'GEN' : 1,\n",
        "    },\n",
        "    'low' : ('GEN',)\n",
        "}\n",
        "\n",
        "non_aug_map = {\n",
        "    'task' : 'Sub-task A',\n",
        "    'map' : {\n",
        "        'NAG' : 0,\n",
        "        'CAG' : 1,\n",
        "        'OAG' : 2,\n",
        "    },\n",
        "    'low' : ('CAG', 'OAG')\n",
        "}\n",
        "\n",
        "train_data_url = 'https://raw.githubusercontent.com/Dutta-SD/NLP/master/Aggression_Detection/trac2_eng_train.csv'"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peKcY1ylxE6V"
      },
      "source": [
        "# Data to augment\n",
        "train = pd.read_csv(train_data_url)\n",
        "train.drop(['ID', non_aug_map['task']], axis = 1, inplace = True)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xRQicfGWm3F"
      },
      "source": [
        "def create_aug_pipeline(model_name : str):\n",
        "  return transformers.pipeline(\"fill-mask\", model_name)"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3gh_bpwsYRUb"
      },
      "source": [
        "def stringCleanerMasker(\n",
        "    ip_string, \n",
        "    stop_words, \n",
        "    num_mask_per_str = 1, \n",
        "    mask_delim = '[MASK]',\n",
        "    max_valid_length = 512,\n",
        "    max_mask_delim_replace = 1):\n",
        "    '''\n",
        "    Takes in a single string, applies a MASK token in some words.\n",
        "    '''\n",
        "\n",
        "    # Remove Stop Words\n",
        "    ip_list = [tok for tok in ip_string.split() if tok not in stop_words]\n",
        "    length = len(ip_list)\n",
        "\n",
        "    if length > max_valid_length:\n",
        "        return 'INVALID'\n",
        "\n",
        "    try:\n",
        "        mask_token = np.random.choice(ip_list, num_mask_per_str)[0]\n",
        "        finalString = ' '.join(ip_list)\n",
        "        finalString = finalString.replace(str(mask_token), mask_delim, max_mask_delim_replace)\n",
        "        return finalString\n",
        "    except Exception as e:\n",
        "        return 'INVALID'"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hKKN-ox9cfbF"
      },
      "source": [
        "# Main Augmentation Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AybadDfXdiJ3"
      },
      "source": [
        "def appendAugDataToDataFrame(\n",
        "    train,\n",
        "    aug_pipe_model_name, \n",
        "    stopwords, \n",
        "    target_col : str,\n",
        "    target_label : str,):\n",
        "    '''\n",
        "    Created Augmented Data using BERT\n",
        "    train : DataFrame - The dataframe to append\n",
        "    target_col : str - name of column to augment\n",
        "    target_label : str - label to augment\n",
        "    '''\n",
        "\n",
        "    subset = (train[target_col] == target_label)\n",
        "    # print(subset.sum())\n",
        "\n",
        "    _data = train[subset]\n",
        "\n",
        "    _text, _labels = _data['Text'], _data[target_col]\n",
        "\n",
        "    # Augmentation Pipeline, \n",
        "    aug_pipe = create_aug_pipeline(aug_pipe_model_name)\n",
        "    \n",
        "    # Augmented Data\n",
        "    aug_data = []\n",
        "    \n",
        "    def stringApp(string, target_label, stopwords, aug_container):\n",
        "        clean_string = stringCleanerMasker(string, stop_words = stopwords)\n",
        "        if clean_string == 'INVALID':\n",
        "            return\n",
        "\n",
        "        # Generate augementations\n",
        "        aug_preds = aug_pipe(clean_string)\n",
        "\n",
        "        for item in aug_preds:\n",
        "            sentence = item['sequence']\n",
        "            aug_container.append((sentence, target_label))\n",
        "\n",
        "        return aug_container\n",
        "\n",
        "    for x, y in tqdm(zip(_text, _labels)):\n",
        "        aug_data.extend(stringApp(x, y, stopwords, []))\n",
        "\n",
        "    aug_data = pd.DataFrame(aug_data, columns = train.columns)\n",
        "    df = train.append(aug_data, ignore_index = True)\n",
        "    return df"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qxH7Dr97eBDz",
        "outputId": "b24cef74-b88f-44e4-f0a6-1933f9088076"
      },
      "source": [
        "# Augmentation\n",
        "train = appendAugDataToDataFrame(\n",
        "    train = train,\n",
        "    aug_pipe_model_name = \"bert-base-multilingual-cased\",\n",
        "    stopwords = stop_words,\n",
        "    target_col = aug_map['task'],\n",
        "    target_label = aug_map['low'][0]\n",
        ")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-multilingual-cased were not used when initializing BertForMaskedLM: ['cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "309it [01:20,  3.85it/s]\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KXhswViRvuHz",
        "outputId": "54dd7b50-4983-4757-c11e-50f2fc45ff63"
      },
      "source": [
        "train[aug_map['task']].value_counts()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "NGEN    3954\n",
              "GEN     1854\n",
              "Name: Sub-task B, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxPDUKmKCNVc"
      },
      "source": [
        "# Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bIaKjfkwpiqY",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "outputId": "97ccdfcf-ff81-4dcc-dd0f-0b615f668ce7"
      },
      "source": [
        "file_name = f\"AUG_{aug_map['task']}_ENGLISH.csv\"\n",
        "\n",
        "train.to_csv(file_name, index = False)\n",
        "\n",
        "files.download(file_name)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "download(\"download_ee72237f-5b18-4a58-aac6-fb3231c12696\", \"AUG_Sub-task B_ENGLISH.csv\", 644703)"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JI726DbFK2bE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}