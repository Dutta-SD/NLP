{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aggression_Detection_Normal_ML_NEW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNXQ7S3oHit0PmuMlah0N9n",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dutta-SD/NLP/blob/sandip-dev/Aggression_Detection/Experiments/Aggression_Detection_Normal_ML_NEW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjcdGfaTw7_s"
      },
      "source": [
        "# Aggression Experiments\n",
        "\n",
        "## CODE NOT FULLY FUNCTIONAL. LAST CELL IS UNDER CONSTRUCTION\n",
        "\n",
        "Aggression_Detection/Experiments/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lCjrxpxEyFtl"
      },
      "source": [
        "! pip install -U -qq demoji"
      ],
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPvnwMvtwff8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b455df26-d95a-42c3-cfee-5c66cbbd8352"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import (\n",
        "    metrics, \n",
        "    ensemble, \n",
        "    svm, \n",
        "    feature_extraction, \n",
        "    naive_bayes, \n",
        "    neural_network, \n",
        "    linear_model,\n",
        "    pipeline,\n",
        "    manifold,\n",
        "    preprocessing,\n",
        "    neighbors,\n",
        "    gaussian_process\n",
        ")\n",
        "import xgboost\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import re\n",
        "import nltk\n",
        "import string\n",
        "import demoji\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 67
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3RHS-DX_iXG"
      },
      "source": [
        "TRAIN_URL_TASK_1 = 'https://raw.githubusercontent.com/Dutta-SD/NLP/master/Aggression_Detection/Aug_Data_Aggression/TASK_A_train_aug_english.csv'\n",
        "TRAIN_URL_TASK_2 = 'https://raw.githubusercontent.com/Dutta-SD/NLP/master/Aggression_Detection/Aug_Data_Aggression/TASK_B_train_aug_english.csv'\n",
        "\n",
        "VAL_URL = 'https://raw.githubusercontent.com/Dutta-SD/NLP/master/Aggression_Detection/trac2_eng_dev.csv'"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHN47dPPAqx3"
      },
      "source": [
        "# Task Description\n",
        "* **A** - Aggression detection\n",
        "* **B** - Misogyny Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "peKcY1ylxE6V",
        "outputId": "3adee62e-51c1-4550-c354-2d0f6c89b6e4"
      },
      "source": [
        "# task 1 - Aggression\n",
        "train = pd.read_csv(TRAIN_URL_TASK_1)\n",
        "val = pd.read_csv(VAL_URL)\n",
        "train.head()"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Sub-task A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Next part</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iii8mllllllm\\nMdxfvb8o90lplppi0005</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ðŸ¤£ðŸ¤£ðŸ˜‚ðŸ˜‚ðŸ¤£ðŸ¤£ðŸ¤£ðŸ˜‚osm vedio ....keep it up...make more v...</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What the fuck was this? I respect shwetabh and...</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Concerned authorities should bring arundathi R...</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text Sub-task A\n",
              "0                                          Next part        NAG\n",
              "1                 Iii8mllllllm\\nMdxfvb8o90lplppi0005        NAG\n",
              "2  ðŸ¤£ðŸ¤£ðŸ˜‚ðŸ˜‚ðŸ¤£ðŸ¤£ðŸ¤£ðŸ˜‚osm vedio ....keep it up...make more v...        NAG\n",
              "3  What the fuck was this? I respect shwetabh and...        NAG\n",
              "4  Concerned authorities should bring arundathi R...        NAG"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjYAqvcrx1vV"
      },
      "source": [
        "def seed_all():\n",
        "  np.random.seed(0)\n",
        "  \n",
        "seed_all()"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TjUUes3TG3b"
      },
      "source": [
        "def clean_one_text(text):\n",
        "    # Cleans one text and returns it    \n",
        "    \n",
        "    # remove punctuation\n",
        "    filter_str = string.punctuation.replace(\"\\'\", \"\")\n",
        "\n",
        "    new_string = text.translate(str.maketrans('', '', filter_str))\n",
        "    tk = nltk.TweetTokenizer()\n",
        "\n",
        "    s = set(nltk.corpus.stopwords.words('english'))\n",
        "    # n't words\n",
        "    rexp_1 = re.compile(r\"n't\")\n",
        "    not_words = set(filter(rexp_1.findall, s))\n",
        "    not_words.update(('against', 'no', 'nor', 'not'))\n",
        "\n",
        "    s.difference_update(not_words)\n",
        "\n",
        "    # stmr = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
        "    stmr = nltk.stem.porter.PorterStemmer()\n",
        "    tokens = [token for token in tk.tokenize(new_string) if token.lower() not in s]\n",
        "    clean_tokens = [stmr.stem(token) for token in tokens]\n",
        "    text = ' '.join(clean_tokens)\n",
        "    # text = demoji.replace_with_desc(text, ':')\n",
        "    return text"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cXvfvCmx5D8"
      },
      "source": [
        "def get_clean_dataset(\n",
        "    df_raw,\n",
        "    target_mapping,\n",
        "    train = True,\n",
        "    task_name='A', \n",
        "    pre_processor=None,\n",
        "    string_cleaner=None,\n",
        "    seed = 0):\n",
        "  '''\n",
        "  ===============================================================\n",
        "  get_clean_dataset - cleans the dataset, returns text and labels\n",
        "  ===============================================================\n",
        "\n",
        "  :df_raw - pandas dataframe for cleaning\n",
        "  :target_mapping - map for the targets\n",
        "  :train - flag to see if training data sent or not\n",
        "  :task_name - the target to predict\n",
        "  :preprocessor - preprocesses the string\n",
        "  :string_cleaner - useful for removing punctuation, etc(function)\n",
        "  '''\n",
        "\n",
        "  seed_all()\n",
        "#   Shuffle\n",
        "  df_raw = df_raw.sample(frac=1).reset_index()\n",
        "\n",
        "  col_str = f'Sub-task {task_name}'\n",
        "\n",
        "  if 'ID' in df_raw.columns:\n",
        "    df_raw = df_raw.drop(['ID'], axis = 1)\n",
        "\n",
        "  targets = df_raw[col_str].map(target_mapping).values\n",
        "  text = df_raw['Text'].values.astype('str')\n",
        "\n",
        "  if string_cleaner is not None:\n",
        "    v_cleaner = np.vectorize(string_cleaner)\n",
        "    text = v_cleaner(text)\n",
        "\n",
        "  if pre_processor:\n",
        "    if train:\n",
        "      text = pre_processor.fit_transform(text)\n",
        "    else:\n",
        "      text = pre_processor.transform(text)  \n",
        "\n",
        "  return text, targets"
      ],
      "execution_count": 72,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO1c1ggheiHo"
      },
      "source": [
        "pp_pipeline = pipeline.Pipeline([\n",
        "               ('fe', feature_extraction.text.TfidfVectorizer()),\n",
        "               ('norm', preprocessing.Normalizer()),\n",
        "               ('pow', preprocessing.QuantileTransformer(output_distribution='normal'))\n",
        "], verbose=True)\n",
        "task_1_map ={\n",
        "    'NAG' : 0,\n",
        "    'CAG' : 1,\n",
        "    'OAG' : 2\n",
        "}"
      ],
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jm-i1Mx7ERH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66329ec5-5d57-4e30-cd40-aae1db831529"
      },
      "source": [
        "X_train, y_train = get_clean_dataset(\n",
        "    df_raw = train,\n",
        "    target_mapping = task_1_map,\n",
        "    train=True,\n",
        "    task_name = 'A',\n",
        "    pre_processor = pp_pipeline,\n",
        "    string_cleaner = clean_one_text\n",
        "    \n",
        ")\n",
        "X_val, y_val = get_clean_dataset(\n",
        "    df_raw = val,\n",
        "    target_mapping = task_1_map,\n",
        "    train=False,\n",
        "    task_name = 'A',\n",
        "    pre_processor = pp_pipeline,\n",
        "    string_cleaner = clean_one_text\n",
        ")"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Pipeline] ................ (step 1 of 3) Processing fe, total=   0.2s\n",
            "[Pipeline] .............. (step 2 of 3) Processing norm, total=   0.0s\n",
            "[Pipeline] ............... (step 3 of 3) Processing pow, total=  10.4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H18s75CJBUyE"
      },
      "source": [
        "# count of objects\n",
        "s = np.bincount(train.iloc[ : , 1].map(task_1_map))\n",
        "\n",
        "weights = {i : s.sum() / s[i] for i in range(len(s))}\n",
        "\n",
        "model1 = ensemble.RandomForestClassifier(max_depth = 3,\n",
        "                                         class_weight=weights, \n",
        "                                         random_state=0, \n",
        "                                        #  criterion='entropy'\n",
        "                                         )\n",
        "model2 = svm.LinearSVC(C = 2, \n",
        "                       class_weight=weights, \n",
        "                       random_state=0,\n",
        "                       )\n",
        "model3 = xgboost.XGBClassifier(max_depth = 4, \n",
        "                               gamma = 0.1 ,\n",
        "                               scale_pos_weight=weights, \n",
        "                               random_state=0,\n",
        "                               n_jobs = -1\n",
        "                               )\n",
        "model4 = neural_network.MLPClassifier(hidden_layer_sizes = (256, 128, 256,64),\n",
        "                                      random_state=0, \n",
        "                                      verbose=2, \n",
        "                                      learning_rate='adaptive', \n",
        "                                      max_iter=20,\n",
        "                                      tol=1e-3\n",
        "                                      )\n",
        "model5 = linear_model.LogisticRegression(\n",
        "    class_weight = weights,\n",
        "    random_state = 0,\n",
        "    max_iter=500,\n",
        "    n_jobs = -1\n",
        "    )\n",
        "model6 = svm.LinearSVC(\n",
        "    random_state=0\n",
        "    )\n",
        "model7 = linear_model.SGDClassifier(\n",
        "    loss = 'log',\n",
        "    random_state=0,\n",
        "    class_weight = weights,\n",
        "    fit_intercept = False\n",
        "    )\n",
        "\n",
        "model_list = [\n",
        "            #   model1, \n",
        "            #   model2, \n",
        "              model3, \n",
        "            #   model4,\n",
        "              model5,\n",
        "            #   model6,\n",
        "            #   model7,\n",
        "            ]"
      ],
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPkCXP2vEwfW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab83ad9e-93f1-4751-e113-7a4d8a611ae4"
      },
      "source": [
        "# Training Loop -- TASK A\n",
        "\n",
        "def train_multiple():\n",
        "    print(f\"Preprocessing used : {type(pp_pipeline).__name__}\", end=\"\\n\\n\")\n",
        "\n",
        "    for i, model in enumerate(model_list):\n",
        "        print(\"-\"*80)\n",
        "        print(f\"MODEL NO : {i}, TRAINING STARTS...\")\n",
        "        print(f\"MODEL NAME : {type(model).__name__}\")\n",
        "        try:\n",
        "            print(f\"MODEL RANDOM SEED(IF ANY) : {model.random_state}\")\n",
        "        except AttributeError:\n",
        "            pass\n",
        "\n",
        "        # Fitting\n",
        "        \n",
        "        preds = model.fit(X_train, y_train).predict(X_val)\n",
        "       \n",
        "        true_preds = y_val\n",
        "\n",
        "\n",
        "        # Validation\n",
        "        print(metrics.classification_report(true_preds, preds))\n",
        "        print(metrics.confusion_matrix(true_preds, preds))\n",
        "\n",
        "        print(f\"\\nDone with model {i}\")\n",
        "\n",
        "train_multiple()"
      ],
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Preprocessing used : Pipeline\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "MODEL NO : 0, TRAINING STARTS...\n",
            "MODEL NAME : XGBClassifier\n",
            "MODEL RANDOM SEED(IF ANY) : 0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.88      0.85      0.86       836\n",
            "           1       0.30      0.41      0.35       117\n",
            "           2       0.49      0.44      0.46       113\n",
            "\n",
            "    accuracy                           0.76      1066\n",
            "   macro avg       0.56      0.57      0.56      1066\n",
            "weighted avg       0.78      0.76      0.77      1066\n",
            "\n",
            "[[709  88  39]\n",
            " [ 55  48  14]\n",
            " [ 41  22  50]]\n",
            "\n",
            "Done with model 0\n",
            "--------------------------------------------------------------------------------\n",
            "MODEL NO : 1, TRAINING STARTS...\n",
            "MODEL NAME : LogisticRegression\n",
            "MODEL RANDOM SEED(IF ANY) : 0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.75      0.80       836\n",
            "           1       0.30      0.43      0.35       117\n",
            "           2       0.29      0.43      0.35       113\n",
            "\n",
            "    accuracy                           0.68      1066\n",
            "   macro avg       0.48      0.54      0.50      1066\n",
            "weighted avg       0.74      0.68      0.71      1066\n",
            "\n",
            "[[629  96 111]\n",
            " [ 57  50  10]\n",
            " [ 43  21  49]]\n",
            "\n",
            "Done with model 1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9rYfFzwf4Qv"
      },
      "source": [
        "# Stacking\n",
        "XGB Gives better result than Stacking. Dropping it"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ov0eVjCf1ZS"
      },
      "source": [
        "# Stacking\n",
        "estimators = [(f'm{i}', model) for i, model in enumerate(model_list)]\n",
        "\n",
        "# Stacking Classifier\n",
        "stk_clf = ensemble.StackingClassifier(\n",
        "    estimators = estimators,\n",
        "    n_jobs = -1,\n",
        "    verbose = True,\n",
        ")\n",
        "\n",
        "# Train\n",
        "stk_clf.fit(X_train, y_train);"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nKI_jWCR9Qj7",
        "outputId": "b626eb79-2d8c-464b-db4f-70798fc4e69f"
      },
      "source": [
        "## Scoring\n",
        "y_preds = stk_clf.predict(X_val)\n",
        "y_true = y_val\n",
        "\n",
        "print(metrics.classification_report(y_true, y_preds))\n",
        "print(metrics.confusion_matrix(y_true, y_preds))"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.86      0.85      0.86       836\n",
            "           1       0.33      0.37      0.35       117\n",
            "           2       0.43      0.40      0.41       113\n",
            "\n",
            "    accuracy                           0.75      1066\n",
            "   macro avg       0.54      0.54      0.54      1066\n",
            "weighted avg       0.76      0.75      0.75      1066\n",
            "\n",
            "[[714  69  53]\n",
            " [ 67  43   7]\n",
            " [ 51  17  45]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBQrZUyW-FQ3"
      },
      "source": [
        "# '''\n",
        "# DO NOT RUN THIS CELL. UNDER CONSTRUCTION\n",
        "# '''\n",
        "\n",
        "# task_2_map ={\n",
        "#     'NGEN' : 0,\n",
        "#     'GEN' : 1,\n",
        "# }\n",
        "# v1 = feature_extraction.text.CountVectorizer()\n",
        "# v2 = feature_extraction.text.TfidfVectorizer()\n",
        "\n",
        "# train_clean = get_clean_dataset(train, True,'B','A', task_2_map, vectorizer=v2)\n",
        "# val_clean = get_clean_dataset(val, False, 'B', 'A', task_2_map, vectorizer=v2)\n",
        "\n",
        "# # print(train_clean[1][1:10])\n",
        "# s_2 = np.bincount(train_clean[1])\n",
        "# # print(s_2)\n",
        "\n",
        "# weights_2 = {i : s_2.sum() / s_2[i] for i in range(2)}\n",
        "# w = s_2[0]/s_2[1]\n",
        "\n",
        "# model1 = ensemble.RandomForestClassifier(class_weight=weights_2, random_state=0, criterion='entropy')\n",
        "# model2 = svm.LinearSVC(class_weight=weights_2, random_state=0)\n",
        "# model3 = xgboost.XGBClassifier(scale_pos_weight=w, random_state=0)\n",
        "# model4 = neural_network.MLPClassifier(random_state=0, verbose=True, learning_rate='adaptive',max_iter=5 )\n",
        "\n",
        "# model_list = [model1, model2, model3, model4]\n",
        "\n",
        "# print(f\"Vectorizer used : {type(v2).__name__}\", end=\"\\n\\n\")\n",
        "\n",
        "# for i, model in enumerate(model_list):\n",
        "#   print(f\"model no {i}, training\")\n",
        "#   print(f\"model name {type(model).__name__}\")\n",
        "#   preds = model.fit(train_clean[0], train_clean[1]).predict(val_clean[0])\n",
        "#   true_preds = val_clean[1]\n",
        "#   print(metrics.classification_report(true_preds, preds))\n",
        "#   print(f\"\\nDone with model {i}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u0BjCRaDDLk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}