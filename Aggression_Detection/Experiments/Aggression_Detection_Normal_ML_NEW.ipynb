{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aggression_Detection.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyONjqSPWRmWLrA3S2lzI1TM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dutta-SD/NLP/blob/master/Aggression_Detection/Experiments/Aggression_Detection_Normal_ML_NEW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjcdGfaTw7_s"
      },
      "source": [
        "# Aggression Experiments\n",
        "\n",
        "## CODE NOT FULLY FUNCTIONAL. LAST CELL IS UNDER CONSTRUCTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPvnwMvtwff8"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import metrics, ensemble, svm, feature_extraction, naive_bayes, neural_network\n",
        "import xgboost\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import re\n",
        "import nltk"
      ],
      "execution_count": 69,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3RHS-DX_iXG"
      },
      "source": [
        "TRAIN_URL_TASK_1 = 'https://raw.githubusercontent.com/Dutta-SD/NLP/master/Aggression_Detection/Aug_Data_Aggression/TASK_A_train_aug_english.csv'\n",
        "TRAIN_URL_TASK_2 = 'https://raw.githubusercontent.com/Dutta-SD/NLP/master/Aggression_Detection/Aug_Data_Aggression/TASK_B_train_aug_english.csv'\n",
        "\n",
        "VAL_URL = 'https://raw.githubusercontent.com/Dutta-SD/NLP/master/Aggression_Detection/trac2_eng_dev.csv'"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHN47dPPAqx3"
      },
      "source": [
        "# Task Description\n",
        "* **A** - Aggression detection\n",
        "* **B** - Misogyny Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "peKcY1ylxE6V",
        "outputId": "1a42d136-150c-4365-b440-29c3c5015ce4"
      },
      "source": [
        "# task 1 - Aggression\n",
        "train = pd.read_csv(TRAIN_URL_TASK_1)\n",
        "val = pd.read_csv(VAL_URL)\n",
        "train.head()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Sub-task A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Next part</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iii8mllllllm\\nMdxfvb8o90lplppi0005</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ðŸ¤£ðŸ¤£ðŸ˜‚ðŸ˜‚ðŸ¤£ðŸ¤£ðŸ¤£ðŸ˜‚osm vedio ....keep it up...make more v...</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What the fuck was this? I respect shwetabh and...</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Concerned authorities should bring arundathi R...</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text Sub-task A\n",
              "0                                          Next part        NAG\n",
              "1                 Iii8mllllllm\\nMdxfvb8o90lplppi0005        NAG\n",
              "2  ðŸ¤£ðŸ¤£ðŸ˜‚ðŸ˜‚ðŸ¤£ðŸ¤£ðŸ¤£ðŸ˜‚osm vedio ....keep it up...make more v...        NAG\n",
              "3  What the fuck was this? I respect shwetabh and...        NAG\n",
              "4  Concerned authorities should bring arundathi R...        NAG"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjYAqvcrx1vV"
      },
      "source": [
        "def seed_all():\n",
        "  np.random.seed(0)\n",
        "  \n",
        "seed_all()"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TjUUes3TG3b"
      },
      "source": [
        "def clean_one_text(text):\n",
        "    # Cleans one text and returns it\n",
        "    \n",
        "    # Clean Punctuation\n",
        "    # Might remove emojis too\n",
        "    # \n",
        "    res = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tk = nltk.TweetTokenizer()\n",
        "    stmr = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
        "    clean_tokens = [stmr.stem(token) for token in tk.tokenize(res)]\n",
        "    return ' '.join(clean_tokens)"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cXvfvCmx5D8"
      },
      "source": [
        "def get_clean_dataset(\n",
        "    df_raw,\n",
        "    target_mapping,\n",
        "    train = True,\n",
        "    task_name='A', \n",
        "    vectorizer=None,\n",
        "    string_cleaner=None,\n",
        "    seed = 0):\n",
        "  '''\n",
        "  ===============================================================\n",
        "  get_clean_dataset - cleans the dataset, returns text and labels\n",
        "  ===============================================================\n",
        "\n",
        "  :df_raw - pandas dataframe for cleaning\n",
        "  :target_mapping - map for the targets\n",
        "  :train - flag to see if training data sent or not\n",
        "  :task_name - the target to predict\n",
        "  :vectorizer - vectorizes the data\n",
        "  :string_cleaner - useful for removing punctuation, etc(function)\n",
        "  '''\n",
        "\n",
        "  col_str = f'Sub-task {task_name}'\n",
        "\n",
        "  if 'ID' in df_raw.columns:\n",
        "    df_raw = df_raw.drop(['ID'], axis = 1)\n",
        "\n",
        "  targets = df_raw[col_str].map(target_mapping).values\n",
        "  text = df_raw['Text'].values.astype('str')\n",
        "\n",
        "  if string_cleaner is not None:\n",
        "    v_cleaner = np.vectorize(string_cleaner)\n",
        "    text = v_cleaner(text)\n",
        "\n",
        "  if vectorizer:\n",
        "    if train:\n",
        "      text = vectorizer.fit_transform(text)\n",
        "    else:\n",
        "      text = vectorizer.transform(text)  \n",
        "\n",
        "  return text, targets"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jm-i1Mx7ERH"
      },
      "source": [
        "X_train, y_train = get_clean_dataset(\n",
        "    df_raw = train,\n",
        "    target_mapping = task_1_map,\n",
        "    train=True,\n",
        "    task_name = 'A',\n",
        "    vectorizer = v2,\n",
        "    string_cleaner = clean_one_text\n",
        "    \n",
        ")\n",
        "X_val, y_val = get_clean_dataset(\n",
        "    df_raw = val,\n",
        "    target_mapping = task_1_map,\n",
        "    train=False,\n",
        "    task_name = 'A',\n",
        "    vectorizer = v2,\n",
        "    string_cleaner = clean_one_text\n",
        ")"
      ],
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H18s75CJBUyE"
      },
      "source": [
        "task_1_map ={\n",
        "    'NAG' : 0,\n",
        "    'CAG' : 1,\n",
        "    'OAG' : 2\n",
        "}\n",
        "\n",
        "# count of objects\n",
        "s = np.bincount(train.iloc[ : , 1].map(task_1_map))\n",
        "\n",
        "# Vectorizers\n",
        "v1 = feature_extraction.text.CountVectorizer()\n",
        "v2 = feature_extraction.text.TfidfVectorizer(strip_accents='unicode')\n",
        "\n",
        "weights = {i : s.sum() / s[i] for i in range(len(s))}\n",
        "\n",
        "model1 = ensemble.RandomForestClassifier(max_depth = 3,\n",
        "                                         class_weight=weights, random_state=0, criterion='entropy')\n",
        "model2 = svm.LinearSVC(C = 0.5, class_weight=weights, random_state=0)\n",
        "model3 = xgboost.XGBClassifier(max_depth = 10, gamma = 0.1 ,scale_pos_weight=weights, random_state=0)\n",
        "model4 = neural_network.MLPClassifier(hidden_layer_sizes = (128, 1024, 128),\n",
        "                                      random_state=0, \n",
        "                                      verbose=True, \n",
        "                                      learning_rate='adaptive', \n",
        "                                      max_iter=20,\n",
        "                                      tol=1e-3)\n",
        "\n",
        "model_list = [\n",
        "              model1, \n",
        "              model2, \n",
        "              model3, \n",
        "            #   model4,\n",
        "              ]"
      ],
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPkCXP2vEwfW",
        "outputId": "60ed6097-05fa-4260-f952-1ae6970b5613"
      },
      "source": [
        "# Training Loop -- TASK A\n",
        "print(f\"Vectorizer used : {type(v2).__name__}\", end=\"\\n\\n\")\n",
        "\n",
        "for i, model in enumerate(model_list):\n",
        "    print(\"-\"*80)\n",
        "    print(f\"MODEL NO : {i}, TRAINING STARTS...\")\n",
        "    print(f\"MODEL NAME : {type(model).__name__}\")\n",
        "    print(f\"MODEL RANDOM SEED(IF ANY) : {model.random_state}\")\n",
        "\n",
        "    # Fitting\n",
        "    preds = model.fit(X_train, y_train).predict(X_val)\n",
        "    true_preds = y_val\n",
        "\n",
        "    # Validation\n",
        "    print(metrics.classification_report(true_preds, preds))\n",
        "    print(metrics.confusion_matrix(true_preds, preds))\n",
        "\n",
        "    print(f\"\\nDone with model {i}\")"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Vectorizer used : TfidfVectorizer\n",
            "\n",
            "--------------------------------------------------------------------------------\n",
            "MODEL NO : 0, TRAINING STARTS...\n",
            "MODEL NAME : RandomForestClassifier\n",
            "MODEL RANDOM SEED(IF ANY) : 0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.87      0.86      0.86       836\n",
            "           1       0.34      0.38      0.36       117\n",
            "           2       0.34      0.32      0.33       113\n",
            "\n",
            "    accuracy                           0.75      1066\n",
            "   macro avg       0.52      0.52      0.52      1066\n",
            "weighted avg       0.75      0.75      0.75      1066\n",
            "\n",
            "[[721  61  54]\n",
            " [ 57  44  16]\n",
            " [ 54  23  36]]\n",
            "\n",
            "Done with model 0\n",
            "--------------------------------------------------------------------------------\n",
            "MODEL NO : 1, TRAINING STARTS...\n",
            "MODEL NAME : LinearSVC\n",
            "MODEL RANDOM SEED(IF ANY) : 0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.87      0.86       836\n",
            "           1       0.31      0.27      0.29       117\n",
            "           2       0.45      0.42      0.44       113\n",
            "\n",
            "    accuracy                           0.76      1066\n",
            "   macro avg       0.54      0.52      0.53      1066\n",
            "weighted avg       0.75      0.76      0.75      1066\n",
            "\n",
            "[[729  58  49]\n",
            " [ 75  32  10]\n",
            " [ 52  13  48]]\n",
            "\n",
            "Done with model 1\n",
            "--------------------------------------------------------------------------------\n",
            "MODEL NO : 2, TRAINING STARTS...\n",
            "MODEL NAME : XGBClassifier\n",
            "MODEL RANDOM SEED(IF ANY) : 0\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.87       836\n",
            "           1       0.29      0.25      0.27       117\n",
            "           2       0.44      0.39      0.42       113\n",
            "\n",
            "    accuracy                           0.76      1066\n",
            "   macro avg       0.53      0.51      0.52      1066\n",
            "weighted avg       0.75      0.76      0.75      1066\n",
            "\n",
            "[[738  57  41]\n",
            " [ 74  29  14]\n",
            " [ 56  13  44]]\n",
            "\n",
            "Done with model 2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBQrZUyW-FQ3"
      },
      "source": [
        "# '''\n",
        "# DO NOT RUN THIS CELL. UNDER CONSTRUCTION\n",
        "# '''\n",
        "\n",
        "# task_2_map ={\n",
        "#     'NGEN' : 0,\n",
        "#     'GEN' : 1,\n",
        "# }\n",
        "# v1 = feature_extraction.text.CountVectorizer()\n",
        "# v2 = feature_extraction.text.TfidfVectorizer()\n",
        "\n",
        "# train_clean = get_clean_dataset(train, True,'B','A', task_2_map, vectorizer=v2)\n",
        "# val_clean = get_clean_dataset(val, False, 'B', 'A', task_2_map, vectorizer=v2)\n",
        "\n",
        "# # print(train_clean[1][1:10])\n",
        "# s_2 = np.bincount(train_clean[1])\n",
        "# # print(s_2)\n",
        "\n",
        "# weights_2 = {i : s_2.sum() / s_2[i] for i in range(2)}\n",
        "# w = s_2[0]/s_2[1]\n",
        "\n",
        "# model1 = ensemble.RandomForestClassifier(class_weight=weights_2, random_state=0, criterion='entropy')\n",
        "# model2 = svm.LinearSVC(class_weight=weights_2, random_state=0)\n",
        "# model3 = xgboost.XGBClassifier(scale_pos_weight=w, random_state=0)\n",
        "# model4 = neural_network.MLPClassifier(random_state=0, verbose=True, learning_rate='adaptive',max_iter=5 )\n",
        "\n",
        "# model_list = [model1, model2, model3, model4]\n",
        "\n",
        "# print(f\"Vectorizer used : {type(v2).__name__}\", end=\"\\n\\n\")\n",
        "\n",
        "# for i, model in enumerate(model_list):\n",
        "#   print(f\"model no {i}, training\")\n",
        "#   print(f\"model name {type(model).__name__}\")\n",
        "#   preds = model.fit(train_clean[0], train_clean[1]).predict(val_clean[0])\n",
        "#   true_preds = val_clean[1]\n",
        "#   print(metrics.classification_report(true_preds, preds))\n",
        "#   print(f\"\\nDone with model {i}\")"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u0BjCRaDDLk"
      },
      "source": [
        ""
      ],
      "execution_count": 18,
      "outputs": []
    }
  ]
}