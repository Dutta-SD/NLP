{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Aggression_Detection_Normal_ML_NEW.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOV1lvWSCd8jaVIbuNrkJaa",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dutta-SD/NLP/blob/master/Aggression_Detection/Experiments/Aggression_Detection_Normal_ML_NEW.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjcdGfaTw7_s"
      },
      "source": [
        "# Aggression Experiments\n",
        "\n",
        "## CODE NOT FULLY FUNCTIONAL. LAST CELL IS UNDER CONSTRUCTION\n",
        "\n",
        "/Aggression_Detection/Experiments/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPvnwMvtwff8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "087e1242-c3a2-49dd-87cc-fb6feb8163b0"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import (\n",
        "    metrics, \n",
        "    ensemble, \n",
        "    svm, \n",
        "    feature_extraction, \n",
        "    naive_bayes, \n",
        "    neural_network, \n",
        "    linear_model,\n",
        ")\n",
        "import xgboost\n",
        "from sklearn.model_selection import cross_val_score\n",
        "import re\n",
        "import nltk\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3RHS-DX_iXG"
      },
      "source": [
        "TRAIN_URL_TASK_1 = 'https://raw.githubusercontent.com/Dutta-SD/NLP/master/Aggression_Detection/Aug_Data_Aggression/TASK_A_train_aug_english.csv'\n",
        "TRAIN_URL_TASK_2 = 'https://raw.githubusercontent.com/Dutta-SD/NLP/master/Aggression_Detection/Aug_Data_Aggression/TASK_B_train_aug_english.csv'\n",
        "\n",
        "VAL_URL = 'https://raw.githubusercontent.com/Dutta-SD/NLP/master/Aggression_Detection/trac2_eng_dev.csv'"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHN47dPPAqx3"
      },
      "source": [
        "# Task Description\n",
        "* **A** - Aggression detection\n",
        "* **B** - Misogyny Detection"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 203
        },
        "id": "peKcY1ylxE6V",
        "outputId": "f3606b02-6b75-4d1f-e920-cc2c0d41c1bb"
      },
      "source": [
        "# task 1 - Aggression\n",
        "train = pd.read_csv(TRAIN_URL_TASK_1)\n",
        "val = pd.read_csv(VAL_URL)\n",
        "train.head()"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Text</th>\n",
              "      <th>Sub-task A</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Next part</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Iii8mllllllm\\nMdxfvb8o90lplppi0005</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>ðŸ¤£ðŸ¤£ðŸ˜‚ðŸ˜‚ðŸ¤£ðŸ¤£ðŸ¤£ðŸ˜‚osm vedio ....keep it up...make more v...</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>What the fuck was this? I respect shwetabh and...</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Concerned authorities should bring arundathi R...</td>\n",
              "      <td>NAG</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                Text Sub-task A\n",
              "0                                          Next part        NAG\n",
              "1                 Iii8mllllllm\\nMdxfvb8o90lplppi0005        NAG\n",
              "2  ðŸ¤£ðŸ¤£ðŸ˜‚ðŸ˜‚ðŸ¤£ðŸ¤£ðŸ¤£ðŸ˜‚osm vedio ....keep it up...make more v...        NAG\n",
              "3  What the fuck was this? I respect shwetabh and...        NAG\n",
              "4  Concerned authorities should bring arundathi R...        NAG"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjYAqvcrx1vV"
      },
      "source": [
        "def seed_all():\n",
        "  np.random.seed(0)\n",
        "  \n",
        "seed_all()"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TjUUes3TG3b"
      },
      "source": [
        "def clean_one_text(text):\n",
        "    # Cleans one text and returns it\n",
        "    \n",
        "    # Clean Punctuation\n",
        "    # Might remove emojis too\n",
        "    # \n",
        "    res = re.sub(r'[^\\w\\s]', '', text)\n",
        "    tk = nltk.TweetTokenizer()\n",
        "\n",
        "    stopwords = set(nltk.corpus.stopwords.words('english'))\n",
        "\n",
        "    # stmr = nltk.stem.snowball.SnowballStemmer(\"english\")\n",
        "    stmr = nltk.stem.porter.PorterStemmer()\n",
        "    tokens = [token for token in tk.tokenize(res) if token.lower() not in stopwords]\n",
        "    clean_tokens = [stmr.stem(token) for token in tokens]\n",
        "    return ' '.join(clean_tokens)"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cXvfvCmx5D8"
      },
      "source": [
        "def get_clean_dataset(\n",
        "    df_raw,\n",
        "    target_mapping,\n",
        "    train = True,\n",
        "    task_name='A', \n",
        "    vectorizer=None,\n",
        "    string_cleaner=None,\n",
        "    seed = 0):\n",
        "  '''\n",
        "  ===============================================================\n",
        "  get_clean_dataset - cleans the dataset, returns text and labels\n",
        "  ===============================================================\n",
        "\n",
        "  :df_raw - pandas dataframe for cleaning\n",
        "  :target_mapping - map for the targets\n",
        "  :train - flag to see if training data sent or not\n",
        "  :task_name - the target to predict\n",
        "  :vectorizer - vectorizes the data\n",
        "  :string_cleaner - useful for removing punctuation, etc(function)\n",
        "  '''\n",
        "\n",
        "  seed_all()\n",
        "#   Shuffle\n",
        "  df_raw = df_raw.sample(frac=1).reset_index()\n",
        "\n",
        "  col_str = f'Sub-task {task_name}'\n",
        "\n",
        "  if 'ID' in df_raw.columns:\n",
        "    df_raw = df_raw.drop(['ID'], axis = 1)\n",
        "\n",
        "  targets = df_raw[col_str].map(target_mapping).values\n",
        "  text = df_raw['Text'].values.astype('str')\n",
        "\n",
        "  if string_cleaner is not None:\n",
        "    v_cleaner = np.vectorize(string_cleaner)\n",
        "    text = v_cleaner(text)\n",
        "\n",
        "  if vectorizer:\n",
        "    if train:\n",
        "      text = vectorizer.fit_transform(text)\n",
        "    else:\n",
        "      text = vectorizer.transform(text)  \n",
        "\n",
        "  return text, targets"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BO1c1ggheiHo"
      },
      "source": [
        "v2 = feature_extraction.text.TfidfVectorizer()\n",
        "task_1_map ={\n",
        "    'NAG' : 0,\n",
        "    'CAG' : 1,\n",
        "    'OAG' : 2\n",
        "}"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Jm-i1Mx7ERH"
      },
      "source": [
        "X_train, y_train = get_clean_dataset(\n",
        "    df_raw = train,\n",
        "    target_mapping = task_1_map,\n",
        "    train=True,\n",
        "    task_name = 'A',\n",
        "    vectorizer = v2,\n",
        "    string_cleaner = clean_one_text\n",
        "    \n",
        ")\n",
        "X_val, y_val = get_clean_dataset(\n",
        "    df_raw = val,\n",
        "    target_mapping = task_1_map,\n",
        "    train=False,\n",
        "    task_name = 'A',\n",
        "    vectorizer = v2,\n",
        "    string_cleaner = clean_one_text\n",
        ")"
      ],
      "execution_count": 55,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H18s75CJBUyE"
      },
      "source": [
        "# count of objects\n",
        "s = np.bincount(train.iloc[ : , 1].map(task_1_map))\n",
        "\n",
        "weights = {i : s.sum() / s[i] for i in range(len(s))}\n",
        "\n",
        "model1 = ensemble.RandomForestClassifier(max_depth = 3,\n",
        "                                         class_weight=weights, random_state=0, criterion='entropy')\n",
        "model2 = svm.LinearSVC(C = 0.5, class_weight=weights, random_state=0)\n",
        "model3 = xgboost.XGBClassifier(max_depth = 4, gamma = 0.1 ,scale_pos_weight=weights, random_state=0)\n",
        "model4 = neural_network.MLPClassifier(hidden_layer_sizes = (128, 1024, 128),\n",
        "                                      random_state=0, \n",
        "                                    #   verbose=True, \n",
        "                                      learning_rate='adaptive', \n",
        "                                      max_iter=10,\n",
        "                                      tol=1e-3)\n",
        "model5 = linear_model.LogisticRegression()\n",
        "model6 = svm.LinearSVC(random_state=0)\n",
        "\n",
        "model_list = [\n",
        "              model1, \n",
        "              model2, \n",
        "              model3, \n",
        "            #   model4,\n",
        "              ]"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GPkCXP2vEwfW"
      },
      "source": [
        "# Training Loop -- TASK A\n",
        "\n",
        "def train_multiple():\n",
        "    print(f\"Vectorizer used : {type(v2).__name__}\", end=\"\\n\\n\")\n",
        "\n",
        "    for i, model in enumerate(model_list):\n",
        "        print(\"-\"*80)\n",
        "        print(f\"MODEL NO : {i}, TRAINING STARTS...\")\n",
        "        print(f\"MODEL NAME : {type(model).__name__}\")\n",
        "        print(f\"MODEL RANDOM SEED(IF ANY) : {model.random_state}\")\n",
        "\n",
        "        # Fitting\n",
        "        preds = model.fit(X_train, y_train).predict(X_val)\n",
        "        true_preds = y_val\n",
        "\n",
        "        # Validation\n",
        "        print(metrics.classification_report(true_preds, preds))\n",
        "        print(metrics.confusion_matrix(true_preds, preds))\n",
        "\n",
        "        print(f\"\\nDone with model {i}\")\n",
        "\n",
        "# train_multiple()"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9rYfFzwf4Qv"
      },
      "source": [
        "# Stacking"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ov0eVjCf1ZS",
        "outputId": "b31c75fd-0740-457c-8adb-2cd34115e96e"
      },
      "source": [
        "# Stacking\n",
        "estimators = [(f'm{i}', model) for i, model in enumerate(model_list)]\n",
        "final_model = model6\n",
        "\n",
        "# Stacking Classifier\n",
        "stk_clf = ensemble.StackingClassifier(\n",
        "    estimators = estimators,\n",
        "    final_estimator = final_model,\n",
        "    n_jobs = -1,\n",
        "    verbose = 10,    \n",
        ")\n",
        "\n",
        "# Train\n",
        "stk_clf.fit(X_train, y_train)\n",
        "\n",
        "y_preds = stk_clf.predict(X_val)\n",
        "y_true = y_val\n",
        "\n",
        "print(metrics.classification_report(y_true, y_preds))\n",
        "print(metrics.confusion_matrix(y_true, y_preds))"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.85      0.88      0.86       836\n",
            "           1       0.28      0.26      0.27       117\n",
            "           2       0.49      0.40      0.44       113\n",
            "\n",
            "    accuracy                           0.76      1066\n",
            "   macro avg       0.54      0.51      0.52      1066\n",
            "weighted avg       0.75      0.76      0.75      1066\n",
            "\n",
            "[[736  61  39]\n",
            " [ 80  30   7]\n",
            " [ 51  17  45]]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/linear_model/_logistic.py:940: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gBQrZUyW-FQ3"
      },
      "source": [
        "# '''\n",
        "# DO NOT RUN THIS CELL. UNDER CONSTRUCTION\n",
        "# '''\n",
        "\n",
        "# task_2_map ={\n",
        "#     'NGEN' : 0,\n",
        "#     'GEN' : 1,\n",
        "# }\n",
        "# v1 = feature_extraction.text.CountVectorizer()\n",
        "# v2 = feature_extraction.text.TfidfVectorizer()\n",
        "\n",
        "# train_clean = get_clean_dataset(train, True,'B','A', task_2_map, vectorizer=v2)\n",
        "# val_clean = get_clean_dataset(val, False, 'B', 'A', task_2_map, vectorizer=v2)\n",
        "\n",
        "# # print(train_clean[1][1:10])\n",
        "# s_2 = np.bincount(train_clean[1])\n",
        "# # print(s_2)\n",
        "\n",
        "# weights_2 = {i : s_2.sum() / s_2[i] for i in range(2)}\n",
        "# w = s_2[0]/s_2[1]\n",
        "\n",
        "# model1 = ensemble.RandomForestClassifier(class_weight=weights_2, random_state=0, criterion='entropy')\n",
        "# model2 = svm.LinearSVC(class_weight=weights_2, random_state=0)\n",
        "# model3 = xgboost.XGBClassifier(scale_pos_weight=w, random_state=0)\n",
        "# model4 = neural_network.MLPClassifier(random_state=0, verbose=True, learning_rate='adaptive',max_iter=5 )\n",
        "\n",
        "# model_list = [model1, model2, model3, model4]\n",
        "\n",
        "# print(f\"Vectorizer used : {type(v2).__name__}\", end=\"\\n\\n\")\n",
        "\n",
        "# for i, model in enumerate(model_list):\n",
        "#   print(f\"model no {i}, training\")\n",
        "#   print(f\"model name {type(model).__name__}\")\n",
        "#   preds = model.fit(train_clean[0], train_clean[1]).predict(val_clean[0])\n",
        "#   true_preds = val_clean[1]\n",
        "#   print(metrics.classification_report(true_preds, preds))\n",
        "#   print(f\"\\nDone with model {i}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8u0BjCRaDDLk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}