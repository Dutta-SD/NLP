{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Agg_and_Misgoyny_Pipeline.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM0zAtOs7nOWum9GSCBjpMz",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Dutta-SD/NLP/blob/master/Aggression_Detection/Experiments/Agg_and_Misgoyny_Pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RjcdGfaTw7_s"
      },
      "source": [
        "# Aggression Experiments\n",
        "\n",
        "Aggression_Detection/Experiments/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cPvnwMvtwff8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5807179c-941a-428d-d940-662488d6ebf9"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn import (\n",
        "    metrics, \n",
        "    ensemble, \n",
        "    feature_extraction, \n",
        "    pipeline,\n",
        "    preprocessing,\n",
        ")\n",
        "import xgboost\n",
        "import nltk\n",
        "import string\n",
        "import re\n",
        "\n",
        "nltk.download('stopwords')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BKA85wPUB32H"
      },
      "source": [
        "# Global Variable Declaration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p3RHS-DX_iXG"
      },
      "source": [
        "BASE_DIR = 'https://raw.githubusercontent.com/Dutta-SD/NLP/master'\n",
        "TRAIN_URL_TASK_1 = f'{BASE_DIR}/Aggression_Detection/Aug_Data_Aggression/TASK_A_train_aug_english.csv'\n",
        "TRAIN_URL_TASK_2 = f'{BASE_DIR}/Aggression_Detection/Aug_Data_Aggression/TASK_B_train_aug_english.csv'\n",
        "\n",
        "VAL_URL = f'{BASE_DIR}/Aggression_Detection/trac2_eng_dev.csv'\n",
        "task_1_map ={\n",
        "    'NAG' : 0,\n",
        "    'CAG' : 1,\n",
        "    'OAG' : 2\n",
        "}\n",
        "\n",
        "task_2_map = {\n",
        "    'NGEN' : 0,\n",
        "    'GEN' : 1\n",
        "}"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aPNv7TBNBfQE"
      },
      "source": [
        "# Utility functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jjYAqvcrx1vV"
      },
      "source": [
        "def seed_all(x):\n",
        "  np.random.seed(x)\n",
        "  \n",
        "seed_all(0)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2TjUUes3TG3b"
      },
      "source": [
        "def clean_one_text(text):\n",
        "    # Cleans one text and returns it    \n",
        "    \n",
        "    # remove punctuation\n",
        "    filter_str = string.punctuation.replace(\"\\'\", \"\")\n",
        "\n",
        "    new_string = text.translate(str.maketrans('', '', filter_str))\n",
        "    tk = nltk.TweetTokenizer()\n",
        "\n",
        "    s = set(nltk.corpus.stopwords.words('english'))\n",
        "    # n't words\n",
        "    rexp_1 = re.compile(r\"n't\")\n",
        "    not_words = set(filter(rexp_1.findall, s))\n",
        "    not_words.update(('against', 'no', 'nor', 'not'))\n",
        "\n",
        "    s.difference_update(not_words)\n",
        "\n",
        "    stmr = nltk.stem.porter.PorterStemmer()\n",
        "    tokens = [token for token in tk.tokenize(new_string) if token.lower() not in s]\n",
        "    clean_tokens = [stmr.stem(token) for token in tokens]\n",
        "    text = ' '.join(clean_tokens)\n",
        "    return text"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9cXvfvCmx5D8"
      },
      "source": [
        "def get_clean_dataset(\n",
        "    df_raw,\n",
        "    target_mapping,\n",
        "    train = True,\n",
        "    task_name='A', \n",
        "    string_cleaner=None,\n",
        "    seed = 0):\n",
        "  '''\n",
        "  ===============================================================\n",
        "  get_clean_dataset - cleans the dataset, returns text and labels\n",
        "  ===============================================================\n",
        "\n",
        "  :df_raw - pandas dataframe for cleaning\n",
        "  :target_mapping - map for the targets\n",
        "  :train - flag to see if training data sent or not\n",
        "  :task_name - the target to predict\n",
        "  :preprocessor - preprocesses the string\n",
        "  :string_cleaner - useful for removing punctuation, etc(function)\n",
        "  '''\n",
        "  \n",
        "  #Shuffle\n",
        "  df_raw = df_raw.sample(frac=1).reset_index()\n",
        "\n",
        "  col_str = f'Sub-task {task_name}'\n",
        "\n",
        "  if 'ID' in df_raw.columns:\n",
        "    df_raw = df_raw.drop(['ID'], axis = 1)\n",
        "\n",
        "  targets = df_raw[col_str].map(target_mapping).values\n",
        "  text = df_raw['Text'].values.astype('str')\n",
        "\n",
        "  if string_cleaner is not None:\n",
        "    v_cleaner = np.vectorize(string_cleaner)\n",
        "    text = v_cleaner(text)\n",
        "\n",
        "  return text, targets"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cNCGEtaAG9Y2"
      },
      "source": [
        "def get_cf_pipe(c_weights, seed = 0):\n",
        "    cf_model = xgboost.XGBClassifier(\n",
        "        max_depth = 4, \n",
        "        gamma = 0.1 ,\n",
        "        scale_pos_weight=c_weights, \n",
        "        random_state=seed,\n",
        "        n_jobs = -1\n",
        "    )\n",
        "    cf_pipe = pipeline.Pipeline(\n",
        "        [\n",
        "            ('feature-extractor', feature_extraction.text.TfidfVectorizer()),\n",
        "            ('norm', preprocessing.Normalizer()),\n",
        "            ('pow-trans', preprocessing.QuantileTransformer(output_distribution='normal')),\n",
        "            ('classifier', cf_model)     \n",
        "        ],\n",
        "        verbose = True\n",
        "    )\n",
        "    return cf_pipe"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "peKcY1ylxE6V"
      },
      "source": [
        "def get_data_and_train_model(\n",
        "    task_name,    \n",
        "    target_map,\n",
        "    string_cleaner,\n",
        "    seed = 0,\n",
        "    verbose = True,\n",
        "    ):\n",
        "    '''\n",
        "    Returns the trained model\n",
        "    '''\n",
        "\n",
        "    # train_data_url\n",
        "    train_url = None\n",
        "    if task_name == 'A':\n",
        "        train_url = TRAIN_URL_TASK_1\n",
        "    else:\n",
        "        train_url = TRAIN_URL_TASK_2\n",
        "    \n",
        "    if verbose:\n",
        "        print('URL SELECTED')    \n",
        "\n",
        "    # Get data\n",
        "    df = pd.read_csv(train_url)\n",
        "\n",
        "    if verbose:\n",
        "        print(f'FETCHED DATA FROM {train_url}')\n",
        "\n",
        "    # Clean data\n",
        "    X_train, y_train = get_clean_dataset(\n",
        "        df,\n",
        "        target_map,\n",
        "        task_name=task_name,\n",
        "        string_cleaner=string_cleaner,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    # Get class weights\n",
        "    weights = np.bincount(y_train)\n",
        "    weights = {i : weights.sum() / weights[i] for i in range(len(weights))}\n",
        "\n",
        "    # Otherwise XGBOOST Throws error\n",
        "    if len(weights) == 2:\n",
        "        weights = weights[1] / weights[0]\n",
        "\n",
        "    if verbose:\n",
        "        print('CLEANING DONE')\n",
        "\n",
        "    cf_pipe = get_cf_pipe(weights, seed=seed)\n",
        "\n",
        "    # Fit the model\n",
        "    if verbose:\n",
        "        print('FITTING...')\n",
        "    cf_pipe.fit(X_train, y_train)\n",
        "\n",
        "    if verbose:\n",
        "        print('TRAINING DONE')    \n",
        "\n",
        "    return {\n",
        "        'model' : cf_pipe,\n",
        "    }"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9dni-2AaEdId"
      },
      "source": [
        "def validate(\n",
        "    clf_pipe,\n",
        "    task_name,\n",
        "    target_map,\n",
        "    string_cleaner,\n",
        "    seed=0,\n",
        "    verbose=True\n",
        "    ):\n",
        "    # Returns Score\n",
        "    if verbose:\n",
        "        print('Validating...')\n",
        "\n",
        "    val_url = VAL_URL\n",
        "    df = pd.read_csv(val_url)\n",
        "\n",
        "    X_val, y_val = get_clean_dataset(\n",
        "        df,\n",
        "        target_map,\n",
        "        train=False,\n",
        "        task_name = task_name,\n",
        "        string_cleaner=string_cleaner,\n",
        "        seed=seed\n",
        "    )\n",
        "\n",
        "    y_preds = clf_pipe.predict(X_val)\n",
        "    y_true = y_val\n",
        "\n",
        "    # Return classification report\n",
        "    return {\n",
        "        'f1_weighted' : metrics.f1_score(y_true, y_preds, average='weighted')\n",
        "    }"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_Ca-DPeIWeq"
      },
      "source": [
        "# Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iv3lPaa9I0Zj",
        "outputId": "02048936-6ca1-494b-9372-9b1c12662430"
      },
      "source": [
        "# Train for task 1\n",
        "# Task - 1\n",
        "# Train\n",
        "clf_pipe_task_1 = get_data_and_train_model(\n",
        "    'A',\n",
        "    task_1_map,\n",
        "    clean_one_text,\n",
        ")['model']\n",
        "\n",
        "# Validate\n",
        "validate(\n",
        "    clf_pipe_task_1,\n",
        "    'A',\n",
        "    task_1_map,\n",
        "    clean_one_text,\n",
        ")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "URL SELECTED\n",
            "FETCHED DATA FROM https://raw.githubusercontent.com/Dutta-SD/NLP/master/Aggression_Detection/Aug_Data_Aggression/TASK_A_train_aug_english.csv\n",
            "CLEANING DONE\n",
            "FITTING...\n",
            "[Pipeline] . (step 1 of 4) Processing feature-extractor, total=   0.2s\n",
            "[Pipeline] .............. (step 2 of 4) Processing norm, total=   0.0s\n",
            "[Pipeline] ......... (step 3 of 4) Processing pow-trans, total=  10.3s\n",
            "[Pipeline] ........ (step 4 of 4) Processing classifier, total=   6.8s\n",
            "TRAINING DONE\n",
            "Validating...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'f1_weighted': 0.7650582678981483}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8KPfM9PLLsL5",
        "outputId": "961a3378-b0ba-423e-a2a4-1054508869ea"
      },
      "source": [
        "# Train for task 2\n",
        "# Task - 2\n",
        "# Train\n",
        "clf_pipe_task_2 = get_data_and_train_model(\n",
        "    'B',\n",
        "    task_2_map,\n",
        "    clean_one_text,\n",
        ")['model']\n",
        "\n",
        "# Validate\n",
        "validate(\n",
        "    clf_pipe_task_2,\n",
        "    'B',\n",
        "    task_2_map,\n",
        "    clean_one_text,\n",
        ")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "URL SELECTED\n",
            "FETCHED DATA FROM https://raw.githubusercontent.com/Dutta-SD/NLP/master/Aggression_Detection/Aug_Data_Aggression/TASK_B_train_aug_english.csv\n",
            "CLEANING DONE\n",
            "FITTING...\n",
            "[Pipeline] . (step 1 of 4) Processing feature-extractor, total=   0.1s\n",
            "[Pipeline] .............. (step 2 of 4) Processing norm, total=   0.0s\n",
            "[Pipeline] ......... (step 3 of 4) Processing pow-trans, total=   9.1s\n",
            "[Pipeline] ........ (step 4 of 4) Processing classifier, total=   1.8s\n",
            "TRAINING DONE\n",
            "Validating...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'f1_weighted': 0.9132994360640823}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    }
  ]
}